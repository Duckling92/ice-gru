{"nonlin": {"func": "LeakyReLU"}, "loss_func": "logcosh", "loss_func_weights": [1, 1, 1], "norm": {"norm": "BatchNorm1D", "momentum": 0.9}, "layers": [{"ResBlock": {"input_sizes": [7, 64], "norm": "LayerNorm", "type": "seq"}}, {"RnnBlock": {"n_in": 64, "n_out": 256, "rnn_type": "GRU", "n_parallel": 1, "num_layers": 3, "residual": false, "bidir": true, "dropout": 0.0, "learn_init": true}}, {"ResBlock": {"input_sizes": [512, 512], "norm": "BatchNorm1D", "type": "x"}}, {"Linear": {"input_sizes": [512, 8], "norm_before_nonlin": true}}]}